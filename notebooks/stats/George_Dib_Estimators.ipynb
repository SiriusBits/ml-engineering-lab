{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiriusBits/ml-engineering-lab/blob/main/notebooks/stats/George_Dib_Estimators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "QNx3oF7UOuud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns  # For plotting\n",
        "from scipy import stats as st  # For standard normal distribution values\n",
        "\n",
        "# This is a \"random number generator\" we'll use for simulations etc.\n",
        "RANDOM_STATE = np.random.RandomState(123)"
      ],
      "metadata": {
        "id": "-3upJUdDRSSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI8g8vMDOaVk"
      },
      "source": [
        "You're given a huge jar of Skittles and M&M's, mixed together (yuck!).\n",
        "\n",
        "You randomly pick 10 pieces of candy from the jar, and end up with 4 Skittles\n",
        "and 6 M&M's.\n",
        "\n",
        "**What proportion of the jar do you think are M&M's?**\n",
        "\n",
        "A natural, intuitive, **estimate** for the proportion of M&M's in the jar would be\n",
        "\n",
        "$$\\frac{6}{4 + 6} = 0.6$$\n",
        "\n",
        "But what can we say about this number?\n",
        "How certain are we that if we count _all_ the candy in the jar, the proportion\n",
        "of M&M's will be close to 60%?\n",
        "\n",
        "To answer these questions, we have to think more carefully about how that\n",
        "**0.6** came about, and what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimator"
      ],
      "metadata": {
        "id": "R2B-j9yIO8pi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-gS6fQxOaVl"
      },
      "source": [
        "To better analyze this number, we need to introduce some generalizations—here, mostly in the form of notation.\n",
        "\n",
        "Let's say there are $C$ pieces of candy in the jar and $M$ pieces are\n",
        "M&M's (i.e., there are $C-M$ Skittles in the jar). We'll call the actual\n",
        "proportion of M&M's in the jar $p$, which is exactly $M/C$.\n",
        "\n",
        "Then, taking out one candy from the jar and inspecting whether it's an M&M or\n",
        "Skittle is equivalent to a Bernoulli trial with probability $p$ of \"success\".\n",
        "We can also do this multiple times, and each time would be an independent\n",
        "Bernoulli trial, as long as we put the candy back in the jar every time.\n",
        "\n",
        "For each piece of candy that we inspect, let's write a 1 if it's an M&M and a\n",
        "0 if it's a Skittle, and we'll call the $i^\\textrm{th}$ number we write $X_i$.\n",
        "Then we can say that the **0.6** above comes from using the following **estimator**:\n",
        "\n",
        "$$\\hat{p} = \\frac{1}{N}\\sum_{i = 1}^{N}X_i$$\n",
        "\n",
        "with our observed data ($X_1, X_2, \\cdots X_{10}$), where $N$ is the number of\n",
        "pieces of candy we get to count.\n",
        "\n",
        "\n",
        "Now, to answer questions regarding our (un)certainty about that **0.6**\n",
        "number, we need to figure out the **distribution** of our estimator,\n",
        "$\\hat{p}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution of an estimator"
      ],
      "metadata": {
        "id": "UI-qAKfaPc4j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd1Hs559OaVo"
      },
      "source": [
        "### A simulation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into distributions with more notation, symbols, and all that,\n",
        "let's think more carefully about what \"**the distribution of** $\\hat{p}$\"\n",
        "actually means. For this, we'll pretend we know exactly how many M&M's and\n",
        "Skittles are in the jar, and see what happens when we compute values of $\\hat{p}$.\n",
        "\n",
        "First, let's create that jar:"
      ],
      "metadata": {
        "id": "HfpX_xX_S0A1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur9IOWaJOaVo"
      },
      "outputs": [],
      "source": [
        "M = 1000   # Let's say this is the number of  M&M's\n",
        "S = 3000   # and this is the number of Skittles.\n",
        "C = M + S  # Then this is the total\n",
        "print(C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1C5z0TOaVo"
      },
      "source": [
        "and the actual proportion of M&M's ($p$) is simply:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwrr_3hVOaVo"
      },
      "outputs": [],
      "source": [
        "p = M/C\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMa-dJ9OaVo"
      },
      "source": [
        "Then, randomly inspecting 10 pieces of candy from the jar is equivalent to sampling 10 times with a probability $p$ of choosing an M&M (`m`) vs. Skittles (`s`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0ClyLhDOaVo"
      },
      "outputs": [],
      "source": [
        "N = 10  # This is how many we get to count\n",
        "\n",
        "counted_candies = RANDOM_STATE.choice(a=[1, 0], p=[p, 1-p], size=N)\n",
        "print(counted_candies)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✋**Question**: How many M&M does the above sample has?"
      ],
      "metadata": {
        "id": "b6X_AHe2WqX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "?RANDOM_STATE.choice"
      ],
      "metadata": {
        "id": "28X5ZOepcCDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWN_fxK7OaVo"
      },
      "source": [
        "Remember, drawing an M&M is considered a \"success\" in this setting, so we can compute our estimator $\\hat{p}$ with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6cxwf6QOaVp"
      },
      "outputs": [],
      "source": [
        "p_hat = counted_candies.sum()/N\n",
        "print(p_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-NhWpIOaVp"
      },
      "source": [
        "The mean of a vector of 0s and 1s is the proportion of elements in the vector that are 1, so we can also do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5ovV8pFOaVp"
      },
      "outputs": [],
      "source": [
        "p_hat = counted_candies.mean()\n",
        "print(p_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✋ **Question:** Do you think the above estimate `p_hat` is a good estimate? How would you be able to make it better and why?"
      ],
      "metadata": {
        "id": "DkrgFOqYVrSB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQjSVvfoOaVp"
      },
      "source": [
        "But that $\\hat{p}$ is just a number! How do we find its distribution? What does a distribution for $\\hat{p}$ even mean?!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW003aKhOaVp"
      },
      "source": [
        "### Sampling distributions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of an estimator (also commonly known as the sampling\n",
        "distribution) refers to how the values of $\\hat{p}$ would look like\n",
        "**if we were to repeat the counting for a whole bunch of samples (of the same size)**.\n",
        "\n",
        "In other words, if there were multiple parallel universes,\n",
        "what would the proportion of M&M's in each\n",
        "of those samples of 10 candies look like across all the universes?\n",
        "\n",
        "✋**Question**: What is the equivalent of parallel universes in the M&M example?\n",
        "\n",
        "While it might be a little difficult to create a bunch of parallel universes in\n",
        "practice, this can easily be done inside a computer."
      ],
      "metadata": {
        "id": "pEF9KKWuS5J1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall, we can generate a sample of size `N` with probability `p` of getting an M&M (`1`), and subsequently compute an estimate $\\hat{p}$ with:"
      ],
      "metadata": {
        "id": "a8ej7osKTWaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_and_estimate(n):\n",
        "  candies = RANDOM_STATE.choice(a=[1, 0], p=[p, 1-p], size=n)\n",
        "  return candies.mean()\n",
        "\n",
        "sample_and_estimate(n=N)"
      ],
      "metadata": {
        "id": "JQua6X43TVPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can simulate repeating this in `B` parallel universes as:"
      ],
      "metadata": {
        "id": "W7DZgBE3TUAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5MoVksROaVp"
      },
      "outputs": [],
      "source": [
        "B = 1000  # Number of parallel universes to create\n",
        "multiverse_p_hats = np.array([\n",
        "  sample_and_estimate(n=N) for _ in range(B)\n",
        "])\n",
        "print(multiverse_p_hats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHuxiOViOaVp"
      },
      "source": [
        "Now that we have `B` values of $\\hat{p}$ from `B` parallel universes, make a histogram of the $\\hat{p}$ values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cohLRCWOaVp"
      },
      "outputs": [],
      "source": [
        "sns.histplot(multiverse_p_hats)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_This_ is (a sample of) the \"sampling distribution\"!\n",
        "\n",
        "Again, remember: you NEVER get to see this in real life!"
      ],
      "metadata": {
        "id": "jZbsWVYlfp5v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g54joPSOaVp"
      },
      "source": [
        "#### Properties of sampling distributions\n",
        "\n",
        "While we know the true value of $p$ (because we made it up, remember?!), we see that the value of $\\hat{p}$ varies across all `B` universes.\n",
        "\n",
        "However, the _average_ of $\\hat{p}$ (the estimated expected value of $\\hat{p}$) across all those universes is actually not that far from the _true_ $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoXnjBsZOaVp"
      },
      "outputs": [],
      "source": [
        "print(f\"True value of p = {p:.2f}\")\n",
        "mean_p_hats = multiverse_p_hats.mean()  # Estimated expected value of p_hat\n",
        "print(f\"Mean value of p_hat in {B:,} universes = {mean_p_hats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-5wyzF-OaVp"
      },
      "source": [
        "This difference between the actual value of interest ($p$) and the expected\n",
        "value of an estimator (average across multiple universes) is what's known as\n",
        "the **bias of an estimator**:\n",
        "\n",
        "$$\\mathbb{E}_p(\\hat{p}) - p$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Bias of p_hat = {mean_p_hats - p:.2f}\")"
      ],
      "metadata": {
        "id": "Qwclj6aHV5XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can also estimate the standard deviation of our estimator $\\hat{p}$ across\n",
        "multiple universes (often called the estimated **standard error**):"
      ],
      "metadata": {
        "id": "ibB27_8JV6AA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUAr90NVOaVq"
      },
      "outputs": [],
      "source": [
        "multiverse_p_hats.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, this is _NOT_ the standard deviation of some observed data. This is the standard deviation of some _simulated_ values of $\\hat{p}$ across multiple parallel universes!"
      ],
      "metadata": {
        "id": "38n-f5MDWMXk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXxQVHSIOaVq"
      },
      "source": [
        "### The math\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also compute the properties of our estimator $\\hat{p}$ analytically,\n",
        "without simulation.\n",
        "\n",
        "First, we have to start with basic probability, and some notation.\n",
        "\n",
        "Let's say that what we observe is $X_i$, and each of these $X_i$ all follow\n",
        "(independently and identically) a Bernoulli distribution with probability of\n",
        "success $p$.\n",
        "\n",
        "Then, from the [properties of a Bernoulli\n",
        "distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) we know that\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_p(X_i) & = p \\\\\n",
        "\\mathrm{Var}_p(X_i) & = p(1-p) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "where the subscript $_p$ in\n",
        "$\\mathbb{E}_p$ and $\\mathrm{Var}_p$ simply means that the actual value $p$ is fixed (i.e., it's _not_ random).\n",
        "\n",
        "**Linearity of expectation** implies:\n",
        "\n",
        "$$\\mathbb{E} \\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathbb{E}(X_i)$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\mathbb{E} (aX_i)  = a\\mathbb{E}(X_i)$$\n",
        "\n",
        "for some constant $a$.\n",
        "\n",
        "Recall our estimator:\n",
        "\n",
        "$$\\hat{p} = \\frac{1}{N}\\sum_{i = 1}^{N}X_i$$"
      ],
      "metadata": {
        "id": "OoCQyt8hev48"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2DHyt-OaVq"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "> **Using the facts above, derive an expression for $\\mathbb{E}_p(\\hat{p})$.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBIjKUK2OaVq"
      },
      "source": [
        "\\begin{align*}\n",
        "\\mathbb{E}_p(\\hat{p}) & = \\mathbb{E}_p\\left(\\frac{1}{N}\\sum_{i = 1}^{N}X_i\\right) \\\\\n",
        "  & = \\frac{1}{N}\\mathbb{E}_p\\left(\\sum_{i = 1}^{N}X_i\\right) \\\\\n",
        "  & = \\frac{1}{N}\\sum_{i = 1}^{N}\\mathbb{E}_p(X_i) \\\\\n",
        "  & = \\frac{1}{N}\\sum_{i = 1}^{N}p \\\\\n",
        "  & = \\frac{1}{N}Np \\\\\n",
        "  & = p\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJax_Bt9OaVq"
      },
      "source": [
        "#### Bias\n",
        "\n",
        "For the bias, we want to figure out the value of\n",
        "\n",
        "$$\\mathbb{E}_p(\\hat{p}) - p$$\n",
        "\n",
        "$\\mathbb{E}_p(\\hat{p}) - p = 0$, so we'd call $\\hat{p}$ **unbiased**.\n",
        "\n",
        "This is confirmed in our simulation above, where we find that the average of our $\\hat{p}$ estimates across `B` multiverses is very close to the true value of $p$!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **A note on population vs. sample variance**\n",
        "\n",
        "Remember that weird note on population vs. sample variance, and degrees of freedom?\n",
        "\n",
        "Well, it turns out that, for some observations $X_1, X_2, \\dots, X_n$ from some population with variance $\\sigma^2$, if you have estimators\n",
        "\n",
        "$$\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( X_i - \\bar{X} \\right)^2$$\n",
        "\n",
        "you can find that:\n",
        "\n",
        "\n",
        "$$\\mathbb{E}_\\sigma(\\hat{\\sigma}^2) = \\frac{n-1}{n}\\sigma^2$$\n",
        "\n",
        "([proof](https://proofwiki.org/wiki/Bias_of_Sample_Variance))\n",
        "\n",
        "In other words, as an _estimator_ of some population variance, $\\hat{\\sigma}^2$ is biased by a factor of $\\frac{n-1}{n}$.\n",
        "Because $n$ is a constant, using linearity of expectation, we can easily \"adjust\" for this bias by multiplying our estimator by the inverse $\\frac{n}{n-1}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{n}{n-1}\n",
        "\\hat{\\sigma}^2\n",
        "& =\n",
        "\\frac{n}{n-1}\n",
        "\\frac{1}{n} \\sum_{i=1}^n \\left( X_i - \\bar{X} \\right)^2 \\\\\n",
        "& =\n",
        "\\frac{1}{n-1} \\sum_{i=1}^n \\left( X_i - \\bar{X} \\right)^2 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "This is how we arived at the _sample_ variance/standard deviation formula that divide by $n-1$ instead of $n$, and this is an _unbiased_ estimate of some population variance $\\sigma^2$."
      ],
      "metadata": {
        "id": "CSIAiJLnT9RY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8H6PfviOaVq"
      },
      "source": [
        "#### Standard error\n",
        "\n",
        "Again, what's known as the **standard error** is simply the **standard deviation of the sampling distribution**, where the _sampling distribution_ is the distribution of values that our estimator $\\hat{p}$ would take across multiple parallel universes.\n",
        "\n",
        "In other words, we want to know\n",
        "\n",
        "$$\\sqrt{\\mathrm{Var}_p(\\hat{p})}$$\n",
        "\n",
        "In case it's not clear yet, $\\hat{p}$ is a **random variable** that can take multiple values, depending on the exact samples ($X_i$) we happened to get, and $p$ is a real, fixed value (the proportion of M&M's in the jar) that won't change no matter how many candies we sample (unless we eat them)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ_DMlaROaVq"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "> Using the facts that\n",
        "> * The variance of a Bernoulli random variable with probability of success $p$ is $p(1-p)$\n",
        "> * For some random variable $X$ and a constant $a$,  $\\mathrm{Var}(aX) = a^2\\mathrm{Var}(X)$\n",
        "> * For two _independent_ random variables $X$ and $Y$, $\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$\n",
        ">\n",
        "> derive an expression for $\\mathrm{Var}_p(\\hat{p})$, and the standard error  $\\sqrt{\\mathrm{Var}_p(\\hat{p})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRP2I1h_OaVr"
      },
      "source": [
        "\\begin{align*}\n",
        "\\mathrm{Var}_p(\\hat{p})\n",
        "  & = \\mathrm{Var}_p\\left(\\frac{1}{N}\\sum_{i = 1}^{N}X_i\\right) \\\\\n",
        "  & = \\frac{1}{N^2}\\sum_{i = 1}^{N}\\mathrm{Var}_p(X_i) \\\\\n",
        "  & = \\frac{1}{N^2}Np(1-p) \\\\\n",
        "  & = \\frac{p(1-p)}{N}\n",
        "\\end{align*}\n",
        "\n",
        "and the standard error is:\n",
        "\n",
        "$$\\sqrt{\\mathrm{Var}_p(\\hat{p})} = \\sqrt{\\frac{p(1-p)}{N}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydL5EitAOaVr"
      },
      "source": [
        "We can compare our result with the standard deviation of our estimates\n",
        "across multiverses, i.e.,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0sJZt0COaVr"
      },
      "outputs": [],
      "source": [
        "# This is what the theory predicts we'll see\n",
        "se_true = np.sqrt(p * (1 - p) / N)\n",
        "\n",
        "# This is what we get from parallel universes\n",
        "se_simulated = multiverse_p_hats.std()\n",
        "\n",
        "print(f\"Actual standard error: {se_true:.2f}\")\n",
        "print(f\"Simulated standard error: {se_simulated:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu-KKZqUOaVr"
      },
      "source": [
        "✋**Question**: Is it possible to calculate the standard error in practice?\n",
        "\n",
        "we know _only $p$ because we created it out of thin air_.So, in reality, we'd _estimate_ the standard error, by using our estimated value of $\\hat{p}$, in place of the true value $p$. For example, in this case we'd compute\n",
        "\n",
        "$$\\hat{\\mathrm{se}} =  \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s0j7XEoOaVr"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "> Calculate $\\hat{\\mathrm{se}}$—the estimated standard error—for our estimator from the very first example of 6 M&M's in a sample of 10 candies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bREY3I8UOaVr"
      },
      "outputs": [],
      "source": [
        "N = 10\n",
        "p_hat = 6 / 10\n",
        "se_hat = np.sqrt(p_hat * (1 - p_hat) / N)\n",
        "\n",
        "print(f\"{se_hat:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the sampling distribution of an estimator\n",
        "\n",
        "In _general_, it's difficult to analytically come up with a sampling distribution.\n",
        "\n",
        "Here we discuss two popular techniques:\n",
        "\n",
        "1. Central limit theorem (CLT): Analytically convincing yourself that your sampling distribution is normal\n",
        "2. Bootstrap: Computationally simulating your sampling distribution"
      ],
      "metadata": {
        "id": "6u0Peg5vRBzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analytical (using Central Limit Theorem)\n",
        "\n",
        "A very popular theorem that is used as a sledgehammer to solving this \"sampling distribution\" problem is the Central Limit Theorem (CLT), which states:\n",
        "\n",
        "> The sum of independent and identically distributed (i.i.d) random variables tends to a normal distribution as the number of random variables increases\n"
      ],
      "metadata": {
        "id": "oU55AKsAlsRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of variables\n",
        "M = 5\n",
        "# Number of observations\n",
        "N = 300\n",
        "\n",
        "uniform_rvs = RANDOM_STATE.uniform(size=(M, N))\n",
        "\n",
        "# Each row is an independent variable with N observations\n",
        "print(uniform_rvs.shape)\n",
        "uniform_rvs"
      ],
      "metadata": {
        "id": "p-bSCc9mR3cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For example, a histogram of the first variable:\n",
        "sns.histplot(uniform_rvs[0, :])"
      ],
      "metadata": {
        "id": "El5RDoCrh28M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have `M` variables that are independent, and have identical distribution as the one shown above.\n",
        "\n",
        "CLT states---whether you believe it or not!---that the variable which results from adding up these i.i.d. variables will tend to a normal distribution, if the number of variables `M` is large enough!\n",
        "\n",
        "For these `M` variables, we see:"
      ],
      "metadata": {
        "id": "nNa3Of-HiAKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how we add the M variables, to get N observations of the new variable\n",
        "uniform_rvs.sum(axis=0)"
      ],
      "metadata": {
        "id": "0tSPR77KjW-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(uniform_rvs.sum(axis=0))"
      ],
      "metadata": {
        "id": "4GSJ80D4Sqkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try with different values of M and N\n",
        "\n",
        "# Number of variables\n",
        "M = 2\n",
        "# Number of observations\n",
        "N = 5000\n",
        "\n",
        "X = RANDOM_STATE.uniform(size=(M, N))\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "sns.histplot(X.sum(axis=0))"
      ],
      "metadata": {
        "id": "5AVarq66inLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This works for _any_ distribution (with finite variance)\n",
        "\n",
        "M = 30\n",
        "N = 1_000\n",
        "\n",
        "X = RANDOM_STATE.exponential(size=(M, N))\n",
        "\n",
        "sns.histplot(X[0, :])"
      ],
      "metadata": {
        "id": "u2AVsoeqRNwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(X.sum(axis=0))"
      ],
      "metadata": {
        "id": "01l7ARUddIYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computational (using bootstrap)\n",
        "\n"
      ],
      "metadata": {
        "id": "xOl0U0jwmMvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the bootstrap, the goal is to simulate parallel universes, using our\n",
        "observations as an approximation of the population.\n",
        "We can create one universe by resampling from our observed data\n",
        "and then computing our estimate over this new \"sample\".\n",
        "\n",
        "An analogy would be to write down each of the results we got from our draws (M&M or Skittles), put them in a _new_ jar, and then randomly pull out numbers out of\n",
        "that new jar, pretending that it represents the actual jar.\n",
        "\n",
        "We can repeat this process many many times, and use the (pretend) _estimates_ we get from this process to build a _numerical_ approximate of the sampling distribution."
      ],
      "metadata": {
        "id": "LChPCu58cOxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some questions\n",
        "\n",
        "* Should we sample with or without replacement?\n",
        "* How big should each of our samples be?"
      ],
      "metadata": {
        "id": "vXLscyTmcnzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall, we actually know, from our analytical results\n",
        "N = 10\n",
        "p_hat = 6 / 10\n",
        "se_hat = np.sqrt(p_hat * (1 - p_hat) / N)\n",
        "\n",
        "print(f\"{se_hat:.2f}\")"
      ],
      "metadata": {
        "id": "HSnTHGJZlM1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets see if a bootstrap can get reasonably close estimates."
      ],
      "metadata": {
        "id": "hshnlYB8dU6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observed_values = [0] * 4 + [1] * 6\n",
        "observed_values"
      ],
      "metadata": {
        "id": "YIAngIsHdTTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The bootstrap sample size. Can be as large as we want.\n",
        "B = 10_000\n",
        "bs_sample = np.array(\n",
        "    [\n",
        "        # Remember, we want samples of _estimates_, where\n",
        "        # our estimator was the sample mean\n",
        "        RANDOM_STATE.choice(\n",
        "            observed_values,\n",
        "            size=len(observed_values),\n",
        "            replace=True,\n",
        "        ).mean()\n",
        "        for _ in range(B)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "44reXkWTdZnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs_sample"
      ],
      "metadata": {
        "id": "rqcYN7FXmYJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `bs_sample` array now represents a literal/numerical approximation of our sampling distribution, and we can use it to approximate certain properties of our estimator (e.g., standard error)."
      ],
      "metadata": {
        "id": "xfBTXpZSeK8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs_sample.std()"
      ],
      "metadata": {
        "id": "vwkmi-F_eFFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(bs_sample)"
      ],
      "metadata": {
        "id": "mJANlFQyRqgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how we don't make any assumptions about the underlying distribution of our data, or rely on any theorem the dictates the sampling distribution. This is what makes the bootstrap very powerful.\n",
        "\n",
        "... But then what _is_ the primary assumption? Why not always use the bootstrap for EVERYTHING?\n",
        "\n",
        "In practice, these days, many applications actually _do_ use bootstrap for EVERYTHING! But important to note:\n",
        "\n",
        "1. Computationally expensive\n",
        "2. A pretty big assumption is that the data we happen to have \"adequately represents\" the population (vs. CLT, if it applies, only requires finite mean/variance and \"large-ish\" sample size)"
      ],
      "metadata": {
        "id": "XODRAq5QedUO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mD-P58BwewZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}